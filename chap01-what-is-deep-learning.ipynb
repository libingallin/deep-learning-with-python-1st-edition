{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 AI ML DL\n",
    "\n",
    "AI、ML 及 DL 之间的关系：\n",
    "\n",
    "![ai-ml-dl](./figs/chap01-figs/ai_ml_dl.png)\n",
    "\n",
    "\n",
    "\n",
    "## 1.1 人工智能\n",
    "\n",
    "<font color='crimson'>人工智能的简介定义：努力将通常由人类完成的智力任务自动化。</font>\n",
    "\n",
    "\n",
    "AI 是一个综合性的领域，不仅包括机器学习与深度学习，还包括更多不涉及学习的方法。如早期的国际象棋仅包含程序员精心编写的硬编码规则。\n",
    "\n",
    "通过编写足够多的明确规则来处理知识，实现人工智能，这一方法称之为**<font color='red'>符号主义人工智能 (symbolic AI) </font>**。在**专家系统**热潮中，这一方法达到了顶峰。虽然符号主义人工智能能适合用于解决明确的逻辑问题，但难以给出明确的规则来解决更加复杂、模糊的问题（如 cv 和 nlp 等）。于是机器学习来代替符号主义人工智能。\n",
    "\n",
    "\n",
    "\n",
    "## 1.2 机器学习\n",
    "\n",
    "在经典的程序设计（即符号主义人工智能）中，输入的是规则（程序）和需要根据这些规则进行处理的数据，系统的输出是答案。\n",
    "\n",
    "<font color='crimson'>在机器学习中，输入的是数据和从这些数据中预期得到的答案，系统输出的是规则。这些规则随后可用于数据，并使计算机自主生成答案。</font>\n",
    "\n",
    "![machine-learning](./figs/chap01-figs/machine_learning.png)\n",
    "\n",
    "<font color='crimson'>机器学习系统是训练出来的，而不是明确地用程序编写出来的。</font>将某个任务相关的许多 instances 输入到 ML 系统中，它会在这些 instances 中找到统计结构，而从最终找到规将任务自动化。\n",
    "\n",
    "\n",
    "与统计学的不同之处：ML 经常用来处理复杂的大型数据集，而用经典的统计分析（如贝叶斯分析）来处理这种数据集是不现实的。因此，**<font color='blue'>ML (尤其是 DL) 呈现出较少的数据理论，并且是以工程为导向的。这需要上手实践，想法更多靠实践来证明，而不是靠理论推导。</font>**\n",
    "\n",
    "\n",
    "## 1.3 从数据中学习表示\n",
    "\n",
    "为区别 ML 和 DL，需要知道 ML 在做什么。给定包含预期的 instances，ML 将会发现执行一项数据处理任务的规则。需要从 3 个方面来进行 ML：\n",
    "\n",
    "- 输入 instances。如图像分类中的图片；语音识别中的声音文件。\n",
    "\n",
    "\n",
    "- 预期 outputs。如图像分类中的🐱和🐶；语音识别中声音对应的文本。\n",
    "\n",
    "\n",
    "- 衡量算法效果好坏的方法。这一衡量方法为计算算法的当前 outputs 与预期 outputs 的差距。衡量结果是一种反馈信号，用于调节算法的工作方式。这个调节过程就是学习。\n",
    "\n",
    "\n",
    "<font color='crimson'>ML 将输入数据转换为有意义的输出，这是一个从已知的输入和输出中进行“学习”的过程。</font>因此，**<font color='blue'>ML 和 DL 的核心问题在于有意义地转换数据，即在于学习输入数据的有用表示（representation）——这种表示可以让数据更接近预期输出。</font>**\n",
    "\n",
    "<font color='crimson'>表示的核心在于以一种不同的方法来查看数据（即表征数据或将数据编码）</font>。如彩色图片可以编码为 RGB（红-绿-蓝）或者 HSV（色相-饱和度-明度）格式，这是相同数据的不同表示方法。<font color='crimson'>在处理某些任务时，使用某种表示可能会很难，但使用另一种方式会变得比较简单</font>。\n",
    "\n",
    "<font color='crimson'>ML 模型都是为输入数据寻找合适的表示——对数据进行变换，使其更适合手头的任务</font>（分类 or 回归 or ...）。\n",
    "\n",
    "> Example: 对图中黑白点进行分类。通过坐标变换，可以用一条简单的规则就可以做到。在新坐标系中，点的坐标可以看作数据的一种新的表示。\n",
    "\n",
    "> ![example](./figs/chap01-figs/coordinate_change.jpg)\n",
    "\n",
    "这里，人为地定义坐标变换。但是，如果我们尝试系统性地搜索各种可能的坐标变换，并用正确分类的点所占百分比作为反馈信号，那么我们做的就是 ML。<font color='crimson'>ML 中的学习指的是，寻找更好数据表示的自动搜索过程。</font>\n",
    "\n",
    "<font color='crimson'>所有 ML 算法都包括自动寻找这样一种变换: 这种变换可以根据任务将数据转化为更加有用的表示</font>。这些操作可以是坐标变换、线性投影(可能会破坏信息)、平移、 非线性操作 (比如“选择所有 x>0 的点”)，等等。<font color='crimson'>ML 算法在寻找这些变换时通常没有什么创造性，而仅仅是遍历一组预先定义好的操作</font>，这组操作叫作**<font color='red'>假设空间 (hypothesis space)</font>**。\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>machine learning is, technically:</b> searching for useful representations of some input data, within a predefined space of possibilities, using guidance from a feedback signal.\n",
    "    \n",
    "<b>ML 的技术定义</b>：在预先定义好的可能性空间 (假设空间)中，利用反馈信号的指引来寻找输入数据的有用表示。\n",
    "</div>\n",
    "\n",
    "\n",
    "## 1.4 深度学习中的“深度”\n",
    "\n",
    "DL 是 ML 的分支领域：**<font color='blue'>DL 是从数据中学习表示的一种新方法，强调从连续的层 (layer) 中进行学习，这些 layer 对应于越来越有意义的表示。<font color='crimson'>**\n",
    "\n",
    "<font color='crimson'>“深度”并不是指利用这种方法来获得更深层次的理解，而是指一系列连续的表示层。</font>模型中含有多少层，这被称之为模型的**<font color='red'>深度 (depth)</font>。\n",
    "\n",
    "DL 也可称为分层表示学习 (layered representation learning) 和层级表示学习 (hierarchical representations learning)。\n",
    "\n",
    "<font color='crimson'>DL 中通常包含数十个甚至上百个连续的、从训练数据中自动学到的表示层，而其他 ML 方法的重点往往是仅仅学习一两层的数据表示</font>，因此有时也被称为**<font color='red'>浅层学习 (shallow learning)</font>**。\n",
    "    \n",
    "在 DL 中，这些分层表示几乎总是通过神经网络 (neural network) 的模型来学习得到的。NN 的结构是逐层堆叠的。**千万不要将 DL 和生物学联系起来。**\n",
    "\n",
    "![NN for 数字分类](figs/chap01-figs/NN_1.png)\n",
    "\n",
    "![NN for 数字分类](figs/chap01-figs/NN_2.png)\n",
    "\n",
    "这个 NN 将数字图像转换成与原始图像差别越来越大的表示，而其中关于最终结果的信息却越来越丰富。可以将 DL 看作多级信息蒸馏操作：信息穿过过滤器，其纯度越来越高（即对任务的帮助越来越大）。\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>deep learning is, technically:</b> a multistage way to learn data representations.\n",
    "    \n",
    "<b>DL 的技术定义</b>：学习数据的多级表示方法。\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "## 1.5 DL  的工作原理\n",
    "\n",
    "ML 是将输入 (比如图像) 映射到目标 (比如标签“猫”) ，这一过程是通过观察许多输入和目标的示例来完成的。DNN 通过一系列简单的 据变换 (层) 来实现这种输入到目标的映射，而这些数据变换都是通过观察示例学习到的。\n",
    "\n",
    "NN 中每层对输入数据所做的具体操作保存在该层的**<font color='red'>权重 (wight)</font>** 中。每层实现的变换由其权重来**<font color='red'>参数化 (parameterize)</font>**。权重有时也被称为该层的**<font color='red'>参数 (parameter)</font>**。此时，<font color='crimson'>学习是指为 NN 的所有层找到一组权重值，使得 NN 能够将每个输入 instance 与其目标一一对应起来</font>。\n",
    "\n",
    "想要控制 NN 的输出，需要能够观察它，即需要能够衡量该输出与预期值之间的距离。这就是 NN 的**<font color='red'>损失函数 (loss function)</font>** 的任务，也叫**<font color='red'>目标函数 (objective function)</font>**。损失函数的输入是 NN 预测值与真实目标值，然后计算一个距离值，衡量该 NN 在这个 instance 上的效果好坏。\n",
    "\n",
    "DL 利用这个距离值作为反馈信号来对权重进行微调，以降低当前 instance 对应的损失值。这种调节由**<font color='red'>优化器 (optimizer)</font>** 来完成，它实现了反向传播算法，这是 **<font color='blue'>DL 的核心</font>**。\n",
    "\n",
    "![NN flow](figs/chap01-figs/NN_flow.png)\n",
    "\n",
    "\n",
    "## 1.6 DL 的短期与未来\n",
    "\n",
    "Don’t believe the short-term hype, but do believe in the long-term vision. It may take a while for AI to be deployed to its true potential—a potential the full extent of which no one has yet dared to dream—but AI is coming, and it will transform our world in a fantastic way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ML 简史\n",
    "\n",
    "当前工业界所使用的绝不大部分 ML 算法都不是 DL 算法。<font color='crimson'>DL 不一定总是解决问题的正确工具。为避免任务 DL 可以解决所有 ML 问题，唯一的方法就是熟悉其他机器学习方法并在适当的适合进行实践。</font>\n",
    "\n",
    "## 2.1 概率建模 (probabilistic  modeling)\n",
    "\n",
    "概率建模是统计学原理在数据分析中的应用。\n",
    "\n",
    "- **朴素贝叶斯**。假设输入数据的特征都是独立的。这是一个非常强的假设，或者说是“朴素的”假设，其名称正来源于此。\n",
    "\n",
    "\n",
    "- **logistic 回归 (logistic regression, logreg)**。现代 ML 的 \"hello world\"。这是一个分类算法，面对一个数据集，通常会首先尝试这个算法，以便初步熟悉手头的任务。\n",
    "\n",
    "\n",
    "## 2.2 早前的 NN\n",
    "\n",
    "早期的 NN 已经被现代的方法所替代。\n",
    "\n",
    "贝尔实验室于 1989 年第一次成功实现了 NN 的实践应用。Yann LeCun 将 CNN 的早期思想与 BP 算法相结合，并将其应用于手写数字分类问题，由此得到 LeNet 网络。\n",
    "\n",
    "\n",
    "## 2.3 核方法\n",
    "\n",
    "SVM 的目标是通过在属于两个不同类别的两组数据点之间找到良好决策边界 (decision boundary) 来解决分类问题。\n",
    "\n",
    "决策边界可以看作一条直线或一个平面，将训练数据划分为两块空间，分别对应于两个类别。对于新数据点的分类，你只需判断它位于决策边界的哪一侧。\n",
    "\n",
    "**<font color='crimson'>SVM 通过两步来寻找决策边界: </font>**\n",
    "\n",
    "1. 将数据映射到一个新的高维表示，这时决策边界可以用一个超平面来表示。\n",
    "\n",
    "2. 尽量让超平面与每个类别最近的数据点之间的距离最大化，从而计算出良好的决策边界（分割超平面），这一步叫做**间隔最大化 (maximizing margin)**。这样的决策边界可以很好地泛化。\n",
    "\n",
    "要将数据映射到高维表示从而简化问题，这就需要用到**<font color='red'>核技巧 (kernel trick)</font>**。其<font color='crimson'>基本思想</font>是：要想在新的表示空间中找到良好的决策超平面，你不需要在新空间中直接计算点的坐标，只需要在新空间中计算点对之间的距离，而利用核函数 (kernel function) 可以高效地完成这种计算。核函数是一个在计算上能够实现的操作，**将原始空间中的任意两点映射为这两点在目标表示空间中的距离，完全避免了对新表示进行直接计算**。(核方法正是因这一核心思想而得名.)\n",
    "\n",
    "核函数通常是人为选择的， 而不是从数据中学到的——对于 SVM 来说，只有分割超平面是通过学习得到的。\n",
    "\n",
    "<font color='crimson'>SVM 很难扩展到大型数据集，并且在图像分类等感知问题上的效果也不好。SVM 是一种比较浅层的方法，因此要想将其应用于感知问题，首先需要手动提取出有用的表示(特征工程)，这一步骤很难，而且不稳定。</font>\n",
    "\n",
    "\n",
    "## 2.4 DT, RF and Gradient boosting\n",
    "\n",
    "- **Decision Trees.** 可视化与解释都很简单。\n",
    "\n",
    "\n",
    "- **Random Forests.** robust, practical. 适用于各种各样的问题——对于任何浅层的 ML 任务来说，它几乎总是第二好的算法。\n",
    "\n",
    "\n",
    "- **Gradient boosting machine.** 使用了梯度提升方法，通过迭代地训练新模型来专门解决之前模型的弱点，从而改进任何 ML 模型的效果。 将梯度提升技术应用于决策树时，得到的模型与 RF 具有相似的性质，但在绝大多数情况下效果都比 RF 要好。它**可能是目前处理非感知数据最好的算法**之一 (如果非要加个“之一”的话)。和 DL 一样，它也是 Kaggle 竞赛中最常用的技术之一。\n",
    "\n",
    "\n",
    "## 2.5 NN\n",
    "\n",
    "爸爸来了。\n",
    "\n",
    "\n",
    "## 2.6 DL  的不同之处\n",
    "\n",
    "DL 发展如此迅速\n",
    "\n",
    "- 主要原因：在于它在很多问题上都表现出更好的性能。\n",
    "\n",
    "\n",
    "- 让解决问题变得更加简单，因为它将特征工程完全自动化。\n",
    "\n",
    "**<font color='blue'>为什么 ML 不行，而 DL 行？</font>**\n",
    "\n",
    "- <font color='crimson'>ML 技术 (浅层学习) 仅包含将输入数据变换到一两个连续的表示空间，通常使用简单的变换</font>，比如高维非线性投影(SVM)或决策树。但这些技术<font color='crimson'>通常无法得到复杂问题所需要的精确表示</font>。因此，人们<font color='crimson'>必须竭尽全力让初始输入数据更适合用这些方法处理，也必须手动为数据设计好的表示层</font> (**特征工程**)。\n",
    "\n",
    "\n",
    "- 与此相反，DL 完全将这个步骤自动化：<font color='crimson'>利用 DL，可以一次性学习所有特征，而无须自己手动设计</font>。这极大地简化了 ML 工作流程，通常将复杂的多阶段流程替换为一个简单的、<font color='crimson'>端到端</font>的 DL 模型。\n",
    "\n",
    "**<font color='blue'>问题的关键在于有多个连续表示层，但为什么不能重复应用浅层学习方法以实现和 DL 类似的结果呢？</font>**\n",
    "\n",
    "<font color='crimson'>在实践中，如果连续应用浅层学习方法，其收益会随着层数增加迅速降低，因为三层模型中最优的第一表示层并不是单层或双层模型中最优的第一表示层。DL 的变革性在于，模型可以在同一时间共同学习所有表示层，而不是依次连续学习 (贪婪学习)。通过共同的特征学习，一旦模型修改某个内部特征，所有依赖于该特征的其他特征都会相应地自动调节适应，无须人为干预。</font>一切都由单一反馈信号来监督：模型中的每一处变化都是为了最终目标服务。这种方法比贪婪地叠加浅层模型更加强大，因为它可以通过将复杂、 抽象的表示拆解为很多个中间空间 (层) 来学习这些表示，每个中间空间仅仅是前一个空间的简单变换。\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>DL 从数据中进行学习时有 2 个基本特征</b>：第一，通过<b>渐进的、逐层的方式形成越来越复杂的表示</b>；第二，<b>对中间这些渐进的表示共同进行学习，每一层的变化都需要同时考虑上下两层的需要</b>。总之，这两个特征使得深度学习比先前的机器学习方法更加成功。\n",
    "</div>\n",
    "\n",
    "\n",
    "## 2.7 ML  现状\n",
    "\n",
    "Kaggle 上主要有 2 大方法：\n",
    "\n",
    "- Gradient boosting machine. 用于处理结构化数据的问题 (浅层学习问题)。几乎都是 XGBoost。\n",
    "\n",
    "\n",
    "- Deep learning. 用于图像分类等感知问题。\n",
    "\n",
    "\n",
    "## 2.8 Why DL?\n",
    "\n",
    "1. **硬件**\n",
    "\n",
    "2. **数据**\n",
    "\n",
    "3. **算法改进训练**\n",
    "\n",
    "  - 更好的 activation function\n",
    "  \n",
    "  - 更好的 weight-initialization scheme\n",
    "  \n",
    "  - 更好的 optimizer\n",
    "  \n",
    "  - Batch Normalization, 残差连接和深度可分离卷积等\n",
    "\n",
    "\n",
    "DL 领域是靠实验结果而不是理论指导，所以只有当合适的数据和硬件可用于尝试新 idea 时，才可能出现算法上的改进。它是一门工程科学。\n",
    "\n",
    "**<font color=\"crimson\">DL 的主要重要性质：</font>**\n",
    "\n",
    "- **Simplicity.** DL 不需要特征工程，它将复杂的、不稳定的、工程量很大的流程替换为简单的、端到端的可训练模型，这些模型通常只用到五六种不同的张量运算。\n",
    "\n",
    "\n",
    "- **Scalability.** DL 非常适合在 GPU 或 TPU 上并行计算，因此可以充分利用摩尔定律。此外， DL 模型通过对小批量数据进行迭代来训练，因此可以在任意大小的数据集上进行训练。(唯一的瓶颈是可用的并行计算能力，而由于摩尔定律，这一限制会越来越小。)\n",
    "\n",
    "\n",
    "- **Versatility and reusability.** 与 ML 方法不同，DL 模型无须从头开始就可以在附加数据上进行训练，因此可用于连续在线学习，这对于大型生产模型而言是非常重要的特性。此外，训练好的 DL 模型可用于其他用途，因此是可以重复使用的。如，可以将一个对图像分类进行训练的深度学习模型应用于视频处理流程。这样我们可以将以前的工作重新投入到日益复杂和强大的模型中。这也使得 DL 可以适用于较小的数据集。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
